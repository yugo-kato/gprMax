

<!DOCTYPE html>
<html class="writer-html5" lang="ja">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>OpenMP, MPI, and HPC &mdash; gprMax  ドキュメント</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=2aa0fae8"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
      <script src="_static/translations.js?v=4dbe4bdc"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="検索" href="search.html" />
    <link rel="next" title="GPGPU" href="gpu.html" />
    <link rel="prev" title="Scripting the input file" href="python_scripting.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            gprMax
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">はじめに</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="include_readme.html">始める</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">ソフトウェアの機能</a></li>
<li class="toctree-l1"><a class="reference internal" href="gprmodelling.html">地中レーダモデリングのガイダンス</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">gprMaxの使用法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="input.html">入力ファイルのコマンド</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">出力データ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Pythonツール</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="plotting.html">プロット</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">File utilities</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">高度な話題</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="python_scripting.html">Scripting the input file</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">OpenMP, MPI, and HPC</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#openmp">OpenMP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mpi">MPI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#extra-installation-steps-for-mpi-task-farm-usage">Extra installation steps for MPI task farm usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#linux-macos">Linux/macOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#microsoft-windows">Microsoft Windows</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hpc-job-script-examples">HPC job script examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#openmp-example">OpenMP example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-mpi-example">OpenMP/MPI example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-mpi-example-no-spawn">OpenMP/MPI example - no spawn</a></li>
<li class="toctree-l3"><a class="reference internal" href="#job-array-example">Job array example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">GPGPU</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ユーザライブラリ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user_libs_antennas.html">GPR antenna models</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_libs_antenna_patterns.html">Antenna patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_libs_austinman.html">AustinMan/AustinWoman</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_libs_materials.html">Materials</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_libs_opt_taguchi.html">Optimisation - Taguchi's method</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">例</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples_simple_2D.html">Introductory (2D) models</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_antennas.html">Antenna models</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_advanced.html">高度な機能</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ヘルプとサポート</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="faqs.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="screencasts.html">Screencasts &amp; videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding.html">コードの概要</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">精度と性能</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="comparisons_analytical.html">Analytical comparisons</a></li>
<li class="toctree-l1"><a class="reference internal" href="comparisons_numerical.html">Numerical comparisons</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">パフォーマンスベンチマーク</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">付録</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="references.html">参考文献</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">gprMax</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">OpenMP, MPI, and HPC</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/openmp_mpi.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="openmp-mpi-and-hpc">
<span id="openmp-mpi"></span><h1>OpenMP, MPI, and HPC<a class="headerlink" href="#openmp-mpi-and-hpc" title="この見出しへのパーマリンク"></a></h1>
<section id="openmp">
<h2>OpenMP<a class="headerlink" href="#openmp" title="この見出しへのパーマリンク"></a></h2>
<p>The most computationally intensive parts of gprMax, which are the FDTD solver loops, have been parallelised using <a class="reference external" href="http://openmp.org">OpenMP</a> which supports multi-platform shared memory multiprocessing.</p>
<p>By default gprMax will try to determine and use the maximum number of OpenMP threads (usually the number of physical CPU cores) available on your machine. You can override this behaviour in two ways: firstly, gprMax will check to see if the <code class="docutils literal notranslate"><span class="pre">#num_threads</span></code> command is present in your input file; if not, gprMax will check to see if the environment variable <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> is set. This can be useful if you are running gprMax in a High-Performance Computing (HPC) environment where you might not want to use all of the available CPU cores.</p>
</section>
<section id="mpi">
<h2>MPI<a class="headerlink" href="#mpi" title="この見出しへのパーマリンク"></a></h2>
<p>The Message Passing Interface (MPI) has been utilised to implement a simple task farm that can be used to distribute a series of models as independent tasks. This can be useful in many GPR simulations where a B-scan (composed of multiple A-scans) is required. Each A-scan can be task-farmed as a independent model. Within each independent model OpenMP threading will continue to be used (as described above). Overall this creates what is know as a mixed mode OpenMP/MPI job.</p>
<p>By default the MPI task farm functionality is turned off. It can be used with the <code class="docutils literal notranslate"><span class="pre">-mpi</span></code> command line option, which specifies the total number of MPI tasks, i.e. master + workers, for the MPI task farm. This option is most usefully combined with <code class="docutils literal notranslate"><span class="pre">-n</span></code> to allow individual models to be farmed out using a MPI task farm, e.g. to create a B-scan with 60 traces and use MPI to farm out each trace: <code class="docutils literal notranslate"><span class="pre">(gprMax)$</span> <span class="pre">python</span> <span class="pre">-m</span> <span class="pre">gprMax</span> <span class="pre">user_models/cylinder_Bscan_2D.in</span> <span class="pre">-n</span> <span class="pre">60</span> <span class="pre">-mpi</span> <span class="pre">61</span></code>.</p>
<p>Our default MPI task farm implementation (activated using the <code class="docutils literal notranslate"><span class="pre">-mpi</span></code> command line option) makes use of the <a class="reference external" href="https://www.open-mpi.org/doc/current/man3/MPI_Comm_spawn.3.php">MPI spawn mechanism</a>. This is sometimes not supported or properly configured on HPC systems. There is therefore an alternate MPI task farm implementation that does not use the MPI spawn mechanism, and is activated using the <code class="docutils literal notranslate"><span class="pre">--mpi-no-spawn</span></code> command line option. See <a class="reference internal" href="#hpc-script-examples"><span class="std std-ref">examples for usage</span></a>.</p>
<section id="extra-installation-steps-for-mpi-task-farm-usage">
<h3>Extra installation steps for MPI task farm usage<a class="headerlink" href="#extra-installation-steps-for-mpi-task-farm-usage" title="この見出しへのパーマリンク"></a></h3>
<p>The following steps provide guidance on how to install the extra components to allow the MPI task farm functionality with gprMax:</p>
<ol class="arabic simple">
<li><p>Install MPI on your system.</p></li>
</ol>
<section id="linux-macos">
<h4>Linux/macOS<a class="headerlink" href="#linux-macos" title="この見出しへのパーマリンク"></a></h4>
<p>It is recommended to use <a class="reference external" href="http://www.open-mpi.org">OpenMPI</a>.</p>
</section>
<section id="microsoft-windows">
<h4>Microsoft Windows<a class="headerlink" href="#microsoft-windows" title="この見出しへのパーマリンク"></a></h4>
<p>It is recommended to use <a class="reference external" href="https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi">Microsoft MPI</a>. Download and install both the .exe and .msi files.</p>
<ol class="arabic simple" start="2">
<li><p>Install the <code class="docutils literal notranslate"><span class="pre">mpi4py</span></code> Python module. Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment <code class="code docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">gprMax</span></code>. Run <code class="code docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">mpi4py</span></code></p></li>
</ol>
</section>
</section>
</section>
<section id="hpc-job-script-examples">
<span id="hpc-script-examples"></span><h2>HPC job script examples<a class="headerlink" href="#hpc-job-script-examples" title="この見出しへのパーマリンク"></a></h2>
<p>HPC environments usually require jobs to be submitted to a queue using a job script. The following are examples of job scripts for a HPC environment that uses <a class="reference external" href="http://gridscheduler.sourceforge.net/index.html">Open Grid Scheduler/Grid Engine</a>, and are intended as general guidance to help you get started. Using gprMax in an HPC environment is heavily dependent on the configuration of your specific HPC/cluster, e.g. the names of parallel environments (<code class="docutils literal notranslate"><span class="pre">-pe</span></code>) and compiler modules will depend on how they were defined by your system administrator.</p>
<section id="openmp-example">
<h3>OpenMP example<a class="headerlink" href="#openmp-example" title="この見出しへのパーマリンク"></a></h3>
<p><a class="reference download internal" download="" href="_downloads/adc113d986743ea0169cf036c49cd854/gprmax_omp.sh"><code class="xref download docutils literal notranslate"><span class="pre">gprmax_omp.sh</span></code></a></p>
<p>Here is an example of a job script for running models, e.g. A-scans to make a B-scan, one after another on a single cluster node. This is not as beneficial as the OpenMP/MPI example, but it can be a helpful starting point when getting the software running in your HPC environment. The behaviour of most of the variables is explained in the comments in the script.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/sh</span>
<span class="linenos"> 2</span><span class="c1">#####################################################################################</span>
<span class="linenos"> 3</span><span class="c1">### Change to current working directory:</span>
<span class="linenos"> 4</span><span class="c1">#$ -cwd</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="c1">### Specify runtime (hh:mm:ss):</span>
<span class="linenos"> 7</span><span class="c1">#$ -l h_rt=01:00:00</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1">### Email options:</span>
<span class="linenos">10</span><span class="c1">#$ -m ea -M joe.bloggs@email.com</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1">### Parallel environment ($NSLOTS):</span>
<span class="linenos">13</span><span class="c1">#$ -pe sharedmem 16</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1">### Job script name:</span>
<span class="linenos">16</span><span class="c1">#$ -N gprmax_omp.sh</span>
<span class="linenos">17</span><span class="c1">#####################################################################################</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1">### Initialise environment module</span>
<span class="linenos">20</span>.<span class="w"> </span>/etc/profile.d/modules.sh
<span class="linenos">21</span>
<span class="linenos">22</span><span class="c1">### Load and activate Anaconda environment for gprMax, i.e. Python 3 and required packages</span>
<span class="linenos">23</span>module<span class="w"> </span>load<span class="w"> </span>anaconda
<span class="linenos">24</span><span class="nb">source</span><span class="w"> </span>activate<span class="w"> </span>gprMax
<span class="linenos">25</span>
<span class="linenos">26</span><span class="c1">### Set number of OpenMP threads for each gprMax model</span>
<span class="linenos">27</span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">16</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="c1">### Run gprMax with input file</span>
<span class="linenos">30</span><span class="nb">cd</span><span class="w"> </span><span class="nv">$HOME</span>/gprMax
<span class="linenos">31</span>python<span class="w"> </span>-m<span class="w"> </span>gprMax<span class="w"> </span>mymodel.in<span class="w"> </span>-n<span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
<p>In this example 10 models will be run one after another on a single node of the cluster (on this particular cluster a single node has 16 cores/threads available). Each model will be parallelised using 16 OpenMP threads.</p>
</section>
<section id="openmp-mpi-example">
<h3>OpenMP/MPI example<a class="headerlink" href="#openmp-mpi-example" title="この見出しへのパーマリンク"></a></h3>
<p><a class="reference download internal" download="" href="_downloads/a33efdcd4f01d56c3e85131c6bdc2242/gprmax_omp_mpi.sh"><code class="xref download docutils literal notranslate"><span class="pre">gprmax_omp_mpi.sh</span></code></a></p>
<p>Here is an example of a job script for running models, e.g. A-scans to make a B-scan, distributed as independent tasks in a HPC environment using MPI. The behaviour of most of the variables is explained in the comments in the script.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/sh</span>
<span class="linenos"> 2</span><span class="c1">#####################################################################################</span>
<span class="linenos"> 3</span><span class="c1">### Change to current working directory:</span>
<span class="linenos"> 4</span><span class="c1">#$ -cwd</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="c1">### Specify runtime (hh:mm:ss):</span>
<span class="linenos"> 7</span><span class="c1">#$ -l h_rt=01:00:00</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1">### Email options:</span>
<span class="linenos">10</span><span class="c1">#$ -m ea -M joe.bloggs@email.com</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1">### Resource reservation:</span>
<span class="linenos">13</span><span class="c1">#$ -R y</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1">### Parallel environment ($NSLOTS):</span>
<span class="linenos">16</span><span class="c1">#$ -pe mpi 176</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="c1">### Job script name:</span>
<span class="linenos">19</span><span class="c1">#$ -N gprmax_omp_mpi.sh</span>
<span class="linenos">20</span><span class="c1">#####################################################################################</span>
<span class="linenos">21</span>
<span class="linenos">22</span><span class="c1">### Initialise environment module</span>
<span class="linenos">23</span>.<span class="w"> </span>/etc/profile.d/modules.sh
<span class="linenos">24</span>
<span class="linenos">25</span><span class="c1">### Load and activate Anaconda environment for gprMax, i.e. Python 3 and required packages</span>
<span class="linenos">26</span>module<span class="w"> </span>load<span class="w"> </span>anaconda
<span class="linenos">27</span><span class="nb">source</span><span class="w"> </span>activate<span class="w"> </span>gprMax
<span class="linenos">28</span>
<span class="linenos">29</span><span class="c1">### Load OpenMPI</span>
<span class="linenos">30</span>module<span class="w"> </span>load<span class="w"> </span>openmpi
<span class="linenos">31</span>
<span class="linenos">32</span><span class="c1">### Set number of OpenMP threads per MPI task (each gprMax model)</span>
<span class="linenos">33</span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">16</span>
<span class="linenos">34</span>
<span class="linenos">35</span><span class="c1">### Run gprMax with input file</span>
<span class="linenos">36</span><span class="nb">cd</span><span class="w"> </span><span class="nv">$HOME</span>/gprMax
<span class="linenos">37</span>python<span class="w"> </span>-m<span class="w"> </span>gprMax<span class="w"> </span>mymodel.in<span class="w"> </span>-n<span class="w"> </span><span class="m">10</span><span class="w"> </span>-mpi<span class="w"> </span><span class="m">11</span>
</pre></div>
</div>
<p>In this example 10 models will be distributed as independent tasks in a HPC environment using MPI.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">-mpi</span></code> argument is passed to gprMax which takes the number of MPI tasks to run. This should be the number of models (worker tasks) plus one extra for the master task.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">NSLOTS</span></code> variable which is required to set the total number of slots/cores for the parallel environment <code class="docutils literal notranslate"><span class="pre">-pe</span> <span class="pre">mpi</span></code> is usually the number of MPI tasks multiplied by the number of OpenMP threads per task. In this example the number of MPI tasks is 11 and number of OpenMP threads per task is 16, so 176 slots are required.</p>
</section>
<section id="openmp-mpi-example-no-spawn">
<h3>OpenMP/MPI example - no spawn<a class="headerlink" href="#openmp-mpi-example-no-spawn" title="この見出しへのパーマリンク"></a></h3>
<p><a class="reference download internal" download="" href="_downloads/8c4fb9627b4faded6e6511e848c106e1/gprmax_omp_mpi_no_spawn.sh"><code class="xref download docutils literal notranslate"><span class="pre">gprmax_omp_mpi_no_spawn.sh</span></code></a></p>
<p>Here is an example of a job script for running models, e.g. A-scans to make a B-scan, distributed as independent tasks in a HPC environment using the MPI implementation without the MPI spawn mechanism. The behaviour of most of the variables is explained in the comments in the script.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/sh</span>
<span class="linenos"> 2</span><span class="c1">#####################################################################################</span>
<span class="linenos"> 3</span><span class="c1">### Change to current working directory:</span>
<span class="linenos"> 4</span><span class="c1">#$ -cwd</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="c1">### Specify runtime (hh:mm:ss):</span>
<span class="linenos"> 7</span><span class="c1">#$ -l h_rt=01:00:00</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1">### Email options:</span>
<span class="linenos">10</span><span class="c1">#$ -m ea -M joe.bloggs@email.com</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1">### Resource reservation:</span>
<span class="linenos">13</span><span class="c1">#$ -R y</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1">### Parallel environment ($NSLOTS):</span>
<span class="linenos">16</span><span class="c1">#$ -pe mpi 176</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="c1">### Job script name:</span>
<span class="linenos">19</span><span class="c1">#$ -N gprmax_omp_mpi_no_spawn.sh</span>
<span class="linenos">20</span><span class="c1">#####################################################################################</span>
<span class="linenos">21</span>
<span class="linenos">22</span><span class="c1">### Initialise environment module</span>
<span class="linenos">23</span>.<span class="w"> </span>/etc/profile.d/modules.sh
<span class="linenos">24</span>
<span class="linenos">25</span><span class="c1">### Load and activate Anaconda environment for gprMax, i.e. Python 3 and required packages</span>
<span class="linenos">26</span>module<span class="w"> </span>load<span class="w"> </span>anaconda
<span class="linenos">27</span><span class="nb">source</span><span class="w"> </span>activate<span class="w"> </span>gprMax
<span class="linenos">28</span>
<span class="linenos">29</span><span class="c1">### Load OpenMPI</span>
<span class="linenos">30</span>module<span class="w"> </span>load<span class="w"> </span>openmpi
<span class="linenos">31</span>
<span class="linenos">32</span><span class="c1">### Set number of OpenMP threads per MPI task (each gprMax model)</span>
<span class="linenos">33</span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">16</span>
<span class="linenos">34</span>
<span class="linenos">35</span><span class="c1">### Run gprMax with input file</span>
<span class="linenos">36</span><span class="nb">cd</span><span class="w"> </span><span class="nv">$HOME</span>/gprMax
<span class="linenos">37</span>mpirun<span class="w"> </span>-n<span class="w"> </span><span class="m">11</span><span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>gprMax<span class="w"> </span>mymodel.in<span class="w"> </span>-n<span class="w"> </span><span class="m">10</span><span class="w"> </span>--mpi-no-spawn
</pre></div>
</div>
<p>In this example 10 models will be distributed as independent tasks in a HPC environment using the MPI implementation without the MPI spawn mechanism.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--mpi-no-spawn</span></code> flag is passed to gprMax which ensures the MPI implementation without the MPI spawn mechanism is used. The number of MPI tasks, i.e. number of models (worker tasks) plus one extra for the master task, should be passed as an argument (<code class="docutils literal notranslate"><span class="pre">-n</span></code>) to the <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> or <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">NSLOTS</span></code> variable which is required to set the total number of slots/cores for the parallel environment <code class="docutils literal notranslate"><span class="pre">-pe</span> <span class="pre">mpi</span></code> is usually the number of MPI tasks multiplied by the number of OpenMP threads per task. In this example the number of MPI tasks is 11 and number of OpenMP threads per task is 16, so 176 slots are required.</p>
</section>
<section id="job-array-example">
<h3>Job array example<a class="headerlink" href="#job-array-example" title="この見出しへのパーマリンク"></a></h3>
<p><a class="reference download internal" download="" href="_downloads/aeed6ddb54fa878e67a538f0b5867ece/gprmax_omp_jobarray.sh"><code class="xref download docutils literal notranslate"><span class="pre">gprmax_omp_jobarray.sh</span></code></a></p>
<p>Here is an example of a job script for running models, e.g. A-scans to make a B-scan, using the job array functionality of Open Grid Scheduler/Grid Engine. A job array is a single submit script that is run multiple times. It has similar functionality, for gprMax, to using the aforementioned MPI task farm. The behaviour of most of the variables is explained in the comments in the script.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/sh</span>
<span class="linenos"> 2</span><span class="c1">#####################################################################################</span>
<span class="linenos"> 3</span><span class="c1">### Change to current working directory:</span>
<span class="linenos"> 4</span><span class="c1">#$ -cwd</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="c1">### Specify runtime (hh:mm:ss):</span>
<span class="linenos"> 7</span><span class="c1">#$ -l h_rt=01:00:00</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1">### Parallel environment ($NSLOTS):</span>
<span class="linenos">10</span><span class="c1">#$ -pe sharedmem 16</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1">### Job array and task IDs</span>
<span class="linenos">13</span><span class="c1">#$ -t 1-11</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1">### Job script name:</span>
<span class="linenos">16</span><span class="c1">#$ -N gprmax_omp_jobarray.sh</span>
<span class="linenos">17</span><span class="c1">#####################################################################################</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1">### Initialise environment module</span>
<span class="linenos">20</span>.<span class="w"> </span>/etc/profile.d/modules.sh
<span class="linenos">21</span>
<span class="linenos">22</span><span class="c1">### Load and activate Anaconda environment for gprMax, i.e. Python 3 and required packages</span>
<span class="linenos">23</span>module<span class="w"> </span>load<span class="w"> </span>anaconda
<span class="linenos">24</span><span class="nb">source</span><span class="w"> </span>activate<span class="w"> </span>gprMax
<span class="linenos">25</span>
<span class="linenos">26</span><span class="c1">### Set number of OpenMP threads for each gprMax model</span>
<span class="linenos">27</span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">16</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="c1">### Run gprMax with input file</span>
<span class="linenos">30</span><span class="nb">cd</span><span class="w"> </span><span class="nv">$HOME</span>/gprMax
<span class="linenos">31</span>python<span class="w"> </span>-m<span class="w"> </span>gprMax<span class="w"> </span>mymodel.in<span class="w"> </span>-n<span class="w"> </span><span class="m">10</span><span class="w"> </span>-task<span class="w"> </span><span class="nv">$SGE_TASK_ID</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">-t</span></code> tells Grid Engine that we are using a job array followed by a range of integers which will be the IDs for each individual task (model). Task IDs must start from 1, and the total number of tasks in the range should correspond to the number of models you want to run, i.e. the integer with the <code class="docutils literal notranslate"><span class="pre">-n</span></code> flag passed to gprMax. The <code class="docutils literal notranslate"><span class="pre">-task</span></code> flag is passed to gprMax to tell it we are using a job array, along with the specific number of the task (model) with the environment variable <code class="docutils literal notranslate"><span class="pre">$SGE_TASK_ID</span></code>.</p>
<p>A job array means that exactly the same submit script is going to be run multiple times, the only difference between each run is the environment variable <code class="docutils literal notranslate"><span class="pre">$SGE_TASK_ID</span></code>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="python_scripting.html" class="btn btn-neutral float-left" title="Scripting the input file" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gpu.html" class="btn btn-neutral float-right" title="GPGPU" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2015-2025, The University of Edinburgh, United Kingdom. Authors: Craig Warren and Antonis Giannopoulos.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>