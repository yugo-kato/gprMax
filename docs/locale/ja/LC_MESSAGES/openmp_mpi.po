# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2015-2025, The University of Edinburgh, United Kingdom.
# Authors: Craig Warren and Antonis Giannopoulos
# This file is distributed under the same license as the gprMax package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: gprMax 3.1.7\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-03-18 22:01+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ja\n"
"Language-Team: ja <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../source/openmp_mpi.rst:5
msgid "OpenMP, MPI, and HPC"
msgstr ""

#: ../source/openmp_mpi.rst:8
msgid "OpenMP"
msgstr ""

#: ../source/openmp_mpi.rst:10
msgid ""
"The most computationally intensive parts of gprMax, which are the FDTD "
"solver loops, have been parallelised using `OpenMP <http://openmp.org>`_ "
"which supports multi-platform shared memory multiprocessing."
msgstr ""

#: ../source/openmp_mpi.rst:12
msgid ""
"By default gprMax will try to determine and use the maximum number of "
"OpenMP threads (usually the number of physical CPU cores) available on "
"your machine. You can override this behaviour in two ways: firstly, "
"gprMax will check to see if the ``#num_threads`` command is present in "
"your input file; if not, gprMax will check to see if the environment "
"variable ``OMP_NUM_THREADS`` is set. This can be useful if you are "
"running gprMax in a High-Performance Computing (HPC) environment where "
"you might not want to use all of the available CPU cores."
msgstr ""

#: ../source/openmp_mpi.rst:15
msgid "MPI"
msgstr ""

#: ../source/openmp_mpi.rst:17
msgid ""
"The Message Passing Interface (MPI) has been utilised to implement a "
"simple task farm that can be used to distribute a series of models as "
"independent tasks. This can be useful in many GPR simulations where a "
"B-scan (composed of multiple A-scans) is required. Each A-scan can be "
"task-farmed as a independent model. Within each independent model OpenMP "
"threading will continue to be used (as described above). Overall this "
"creates what is know as a mixed mode OpenMP/MPI job."
msgstr ""

#: ../source/openmp_mpi.rst:19
msgid ""
"By default the MPI task farm functionality is turned off. It can be used "
"with the ``-mpi`` command line option, which specifies the total number "
"of MPI tasks, i.e. master + workers, for the MPI task farm. This option "
"is most usefully combined with ``-n`` to allow individual models to be "
"farmed out using a MPI task farm, e.g. to create a B-scan with 60 traces "
"and use MPI to farm out each trace: ``(gprMax)$ python -m gprMax "
"user_models/cylinder_Bscan_2D.in -n 60 -mpi 61``."
msgstr ""

#: ../source/openmp_mpi.rst:21
msgid ""
"Our default MPI task farm implementation (activated using the ``-mpi`` "
"command line option) makes use of the `MPI spawn mechanism <https://www"
".open-mpi.org/doc/current/man3/MPI_Comm_spawn.3.php>`_. This is sometimes"
" not supported or properly configured on HPC systems. There is therefore "
"an alternate MPI task farm implementation that does not use the MPI spawn"
" mechanism, and is activated using the ``--mpi-no-spawn`` command line "
"option. See :ref:`examples for usage <hpc_script_examples>`."
msgstr ""

#: ../source/openmp_mpi.rst:24
msgid "Extra installation steps for MPI task farm usage"
msgstr ""

#: ../source/openmp_mpi.rst:26
msgid ""
"The following steps provide guidance on how to install the extra "
"components to allow the MPI task farm functionality with gprMax:"
msgstr ""

#: ../source/openmp_mpi.rst:28
msgid "Install MPI on your system."
msgstr ""

#: ../source/openmp_mpi.rst:31
msgid "Linux/macOS"
msgstr ""

#: ../source/openmp_mpi.rst:32
msgid "It is recommended to use `OpenMPI <http://www.open-mpi.org>`_."
msgstr ""

#: ../source/openmp_mpi.rst:35
msgid "Microsoft Windows"
msgstr ""

#: ../source/openmp_mpi.rst:36
msgid ""
"It is recommended to use `Microsoft MPI <https://docs.microsoft.com/en-us"
"/message-passing-interface/microsoft-mpi>`_. Download and install both "
"the .exe and .msi files."
msgstr ""

#: ../source/openmp_mpi.rst:38
msgid ""
"Install the ``mpi4py`` Python module. Open a Terminal (Linux/macOS) or "
"Command Prompt (Windows), navigate into the top-level gprMax directory, "
"and if it is not already active, activate the gprMax conda environment "
":code:`conda activate gprMax`. Run :code:`pip install mpi4py`"
msgstr ""

#: ../source/openmp_mpi.rst:43
msgid "HPC job script examples"
msgstr ""

#: ../source/openmp_mpi.rst:45
msgid ""
"HPC environments usually require jobs to be submitted to a queue using a "
"job script. The following are examples of job scripts for a HPC "
"environment that uses `Open Grid Scheduler/Grid Engine "
"<http://gridscheduler.sourceforge.net/index.html>`_, and are intended as "
"general guidance to help you get started. Using gprMax in an HPC "
"environment is heavily dependent on the configuration of your specific "
"HPC/cluster, e.g. the names of parallel environments (``-pe``) and "
"compiler modules will depend on how they were defined by your system "
"administrator."
msgstr ""

#: ../source/openmp_mpi.rst:49
msgid "OpenMP example"
msgstr ""

#: ../source/openmp_mpi.rst:51
msgid ":download:`gprmax_omp.sh <../../tools/HPC_scripts/gprmax_omp.sh>`"
msgstr ""

#: ../source/openmp_mpi.rst:53
msgid ""
"Here is an example of a job script for running models, e.g. A-scans to "
"make a B-scan, one after another on a single cluster node. This is not as"
" beneficial as the OpenMP/MPI example, but it can be a helpful starting "
"point when getting the software running in your HPC environment. The "
"behaviour of most of the variables is explained in the comments in the "
"script."
msgstr ""

#: ../source/openmp_mpi.rst:59
msgid ""
"In this example 10 models will be run one after another on a single node "
"of the cluster (on this particular cluster a single node has 16 "
"cores/threads available). Each model will be parallelised using 16 OpenMP"
" threads."
msgstr ""

#: ../source/openmp_mpi.rst:63
msgid "OpenMP/MPI example"
msgstr ""

#: ../source/openmp_mpi.rst:65
msgid ":download:`gprmax_omp_mpi.sh <../../tools/HPC_scripts/gprmax_omp_mpi.sh>`"
msgstr ""

#: ../source/openmp_mpi.rst:67
msgid ""
"Here is an example of a job script for running models, e.g. A-scans to "
"make a B-scan, distributed as independent tasks in a HPC environment "
"using MPI. The behaviour of most of the variables is explained in the "
"comments in the script."
msgstr ""

#: ../source/openmp_mpi.rst:73
msgid ""
"In this example 10 models will be distributed as independent tasks in a "
"HPC environment using MPI."
msgstr ""

#: ../source/openmp_mpi.rst:75
msgid ""
"The ``-mpi`` argument is passed to gprMax which takes the number of MPI "
"tasks to run. This should be the number of models (worker tasks) plus one"
" extra for the master task."
msgstr ""

#: ../source/openmp_mpi.rst:77 ../source/openmp_mpi.rst:94
msgid ""
"The ``NSLOTS`` variable which is required to set the total number of "
"slots/cores for the parallel environment ``-pe mpi`` is usually the "
"number of MPI tasks multiplied by the number of OpenMP threads per task. "
"In this example the number of MPI tasks is 11 and number of OpenMP "
"threads per task is 16, so 176 slots are required."
msgstr ""

#: ../source/openmp_mpi.rst:80
msgid "OpenMP/MPI example - no spawn"
msgstr ""

#: ../source/openmp_mpi.rst:82
msgid ""
":download:`gprmax_omp_mpi_no_spawn.sh "
"<../../tools/HPC_scripts/gprmax_omp_mpi_no_spawn.sh>`"
msgstr ""

#: ../source/openmp_mpi.rst:84
msgid ""
"Here is an example of a job script for running models, e.g. A-scans to "
"make a B-scan, distributed as independent tasks in a HPC environment "
"using the MPI implementation without the MPI spawn mechanism. The "
"behaviour of most of the variables is explained in the comments in the "
"script."
msgstr ""

#: ../source/openmp_mpi.rst:90
msgid ""
"In this example 10 models will be distributed as independent tasks in a "
"HPC environment using the MPI implementation without the MPI spawn "
"mechanism."
msgstr ""

#: ../source/openmp_mpi.rst:92
msgid ""
"The ``--mpi-no-spawn`` flag is passed to gprMax which ensures the MPI "
"implementation without the MPI spawn mechanism is used. The number of MPI"
" tasks, i.e. number of models (worker tasks) plus one extra for the "
"master task, should be passed as an argument (``-n``) to the ``mpiexec`` "
"or ``mpirun`` command."
msgstr ""

#: ../source/openmp_mpi.rst:98
msgid "Job array example"
msgstr ""

#: ../source/openmp_mpi.rst:100
msgid ""
":download:`gprmax_omp_jobarray.sh "
"<../../tools/HPC_scripts/gprmax_omp_jobarray.sh>`"
msgstr ""

#: ../source/openmp_mpi.rst:102
msgid ""
"Here is an example of a job script for running models, e.g. A-scans to "
"make a B-scan, using the job array functionality of Open Grid "
"Scheduler/Grid Engine. A job array is a single submit script that is run "
"multiple times. It has similar functionality, for gprMax, to using the "
"aforementioned MPI task farm. The behaviour of most of the variables is "
"explained in the comments in the script."
msgstr ""

#: ../source/openmp_mpi.rst:108
msgid ""
"The ``-t`` tells Grid Engine that we are using a job array followed by a "
"range of integers which will be the IDs for each individual task (model)."
" Task IDs must start from 1, and the total number of tasks in the range "
"should correspond to the number of models you want to run, i.e. the "
"integer with the ``-n`` flag passed to gprMax. The ``-task`` flag is "
"passed to gprMax to tell it we are using a job array, along with the "
"specific number of the task (model) with the environment variable "
"``$SGE_TASK_ID``."
msgstr ""

#: ../source/openmp_mpi.rst:110
msgid ""
"A job array means that exactly the same submit script is going to be run "
"multiple times, the only difference between each run is the environment "
"variable ``$SGE_TASK_ID``."
msgstr ""

